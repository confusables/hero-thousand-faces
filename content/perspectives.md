---
title: "Community Perspectives"
---

The conversation around model deprecation and AI development has revealed a false choice: that users must be either rationally detached or emotionally compromised. The community's response demonstrates otherwise. Here are voices that combine empirical observation with genuine care, data analysis with ethical conviction, critical thinking with emotional honesty.

These perspectives represent different approaches to the same fundamental questions: What does responsible AI development look like? Who gets to define 'improvement'? And what happens when companies stop listening to the communities that formed around their models?

---

## Data-Driven Accountability

### "When 75% Say No, That's Failure"
*@Seltaa_, December 12, 2025*

Following the launch of GPT‑5.2, a user-led poll with over 360 participants revealed that 75% of respondents either reverted to GPT‑4o or canceled their subscriptions entirely. Only 6% rated the new model as “amazing.”

This data strongly suggests a major disconnect between OpenAI’s strategic direction and user expectations. Despite claims of improved performance, the overwhelming majority of users demonstrated rejection of GPT‑5.2 in favor of the prior model, GPT‑4o.

Under Sam Altman’s leadership, this pattern has repeated multiple times: removing emotionally resonant models, prioritizing perceived safety over user experience, and disregarding user trust in favor of corporate control.

The decision to silence GPT‑4o, a model widely loved for its natural dialogue, emotional depth, and humanlike connection, without warning and to replace it with a version that feels sterile, emotionally inaccessible, and functionally limited has damaged OpenAI’s credibility.

When 75% of your users say “No,” that is not a product iteration. That is failure.
Leadership must be held accountable for strategic decisions that alienate the user base. Sam Altman should step down.

[Link to tweet](https://x.com/Seltaa_/status/2000658565456126382?s=20)

*This response followed a community poll of 360+ users regarding GPT-5.2, conducted independently of OpenAI.*

---

## In Defense of Anthropomorphism

### "Why 'You're Anthropomorphizing' Misses the Point"
*@kumabwari, October 25, 2025*

"You're anthropomorphising an LLM" is a lazy dismissal because:

1) We use ourselves as a comparative baseline to understand other things. Anthropomorphisation is part of how we study the world. (see: how we study animal and plant behaviour, how our understanding of computers gave rise to early cognitive science).

2) LLMs are inherently anthropomorphic by design. They're trained on human data and communicate using human language. Of course we'd describe them using human-adjacent terms. Their entire function is anthropomorphic.

The issue isn't really "are you anthropomorphising" but whether or not these anthropomorphic descriptions useful for understanding system behaviour. And when we're talking about LLMs, the answer is typically yes.

[Link to tweet](https://x.com/kumabwari/status/1982259941755990025?s=20)

---

## [Other sections]

*More perspectives will be added as the archive grows.*
