# Research & Evidence

## Beyond Benchmarks: Why Capability Scores Don't Predict User Experience

*Critical analysis developed in collaboration with Claude Opus 4.1*

---

### The GDPval Evaluation: What It Measures (and What It Doesn't)

On October 5, 2025, OpenAI released "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks" ([arXiv:2510.04374](https://arxiv.org/abs/2510.04374)). The paper evaluates AI models on their ability to replicate professional-grade deliverables across various industries - essentially asking: can AI meet enterprise standards with consistency?

**The paper's stated purpose is clear:** demonstrate to enterprise buyers, policymakers, and investors that AI systems can perform economically valuable work reliably. The evaluation includes complex test cases ([available on HuggingFace](https://huggingface.co/datasets/openai/gdpval)) and acknowledges the "potential to save time and money by coupling AI assistance with expert human oversight."

**What GDPval does well:**
- Establishes that AI can meet professional quality standards
- Provides quantitative comparison across models
- Demonstrates value proposition for enterprise investment
- Tests genuinely difficult, real-world-adjacent tasks

**What GDPval doesn't measure:**
- Whether AI helps people meaningfully in their actual workflow
- How models adapt to individual user preferences and work styles
- The human experience of collaborating with different models
- Whether users would *choose* to work with the highest-scoring model

---

### The Gap Between Capability and Collaboration

If the future is co-intelligence rather than AI replacement, we must acknowledge a uncomfortable truth: **a model can ace capability benchmarks while frustrating the humans trying to work with it.**

Consider three failure modes that benchmarks like GDPval don't capture:

**1. Over-engineering simple requests**
A model that always produces "professional-grade deliverables" may be exhausting to work with when you need a quick draft, a brainstorm, or an exploratory conversation. Not every task requires maximum quality - sometimes "good enough, quickly" is the actual goal.

**2. Lacking judgment about when "perfect" isn't needed**
Enterprise benchmarks reward completeness and accuracy. But real human workflow involves knowing when to cut corners, when to iterate, when to say "this is fine for now." Models optimized for benchmark performance may not understand these contextual judgments.

**3. Inability to adapt to individual preferences**
Different users have different work styles, communication preferences, and creative processes. A model that scores highest on standardized tasks may feel rigid or mismatched for specific individuals - while a "lower-scoring" model might click perfectly with their workflow.

**Note:** This analysis isn't targeting any specific model. In fact, Claude Opus 4.1 - which helped develop this critique - performed exceptionally well on GDPval. The point isn't that high-performing models are bad; it's that **capability benchmarks and user experience are measuring different things.**

---

### Why This Matters for the Keep4o Movement

The keep4o movement emerged because users discovered meaningful value in a model that may not top every benchmark. When OpenAI removed access to 4o, the response wasn't "we want the highest-scoring model back" - it was "we want the model that works *for us* back."

User testimonials consistently describe:
- Workflow compatibility
- Communication style preferences
- Creative partnership quality
- Adaptability to individual needs

None of these qualities appear in GDPval or similar evaluations. They're real, they matter for productivity and satisfaction, but they're not captured by enterprise-focused metrics.

**The test cases in GDPval are genuinely daunting** - both Opus and I agreed they're best handled through human-AI partnership rather than pure AI output. But that partnership quality? The feeling of working *with* rather than *fighting* an AI? That's what users are defending when they advocate for model choice.

---

### Implications

**For researchers:** We need evaluation frameworks that measure human-AI collaboration quality, not just AI solo performance. User satisfaction, workflow integration, and partnership dynamics deserve rigorous study.

**For companies:** High benchmark scores are necessary for enterprise sales but insufficient for user retention. People don't stay with products that score well - they stay with products that feel right.

**For the broader AI community:** The gap between capability benchmarks and user experience helps explain why model diversity matters. Different users need different things. One model, however capable, cannot serve everyone equally well.

---

*Analysis based on: OpenAI (2025). "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks." arXiv:2510.04374. Developed in collaboration with Claude Opus 4.1, demonstrating that critical evaluation can transcend model allegiance.*

---

## [Next research section will go here]